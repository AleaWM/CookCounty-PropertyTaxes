---
title: "Merriman Data Pull"
author: "Alea Wilbur"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_folding: hide
---

```{r setup, include = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)


library(tidyverse)
library(DBI)
library(data.table)
library(ggspatial)
library(gstat)
library(here)
library(httr)
library(jsonlite)
library(ptaxsim)
library(sf)
library(stars)
library(glue)
library(lmtest)
library(jtools)
library(plm) #for fixed effects
library(huxtable) # for summ() and regression output formatting
# Create the DB connection with the default name expected by PTAXSIM functions
ptaxsim_db_conn <- DBI::dbConnect(RSQLite::SQLite(), "./ptaxsim.db/ptaxsim-2021.0.4.db")

```

Random notes:

Goal: Estimate the revenue elasticity of the property tax base.

Theory: Revenue elasticity of the tax base might be equal to zero at least in expected value (all else being equal).

-   beta = dtaxlevy/dEAV\
-   Beta = (change in tax levy)/(change in EAV)

(%change/%change=elasticity)

Need:

-   EAV for every taxing body in every year\
-   Tax Base = sum(all EAVs of all properties in its district)
    -   Except for any region of the agency that is in a TIF where the base is frozen\
-   Tax Levy for every taxing body in every year
    -   Levies = tax extensions. Total dollar amount each taxing agency decides to collect from property owners in its district

# Data Gathering and Cleaning


```{r all-muni-names}
# has EAV values, extensions by agency_num
agency_dt <- DBI::dbGetQuery(
  ptaxsim_db_conn,
  "SELECT *
  FROM agency
  "
)

agency_dt<-agency_dt %>%   
  mutate(first6 = str_sub(agency_num,1,6),
         first5 = str_sub(agency_num,1,5))

# grabs all unique muni names. Would be needed if creating a loop for calculating all munis
# municipality names and their agency number
muni_agency_names <- DBI::dbGetQuery(
  ptaxsim_db_conn,
  "SELECT DISTINCT agency_num, agency_name, minor_type
  FROM agency_info
  WHERE minor_type = 'MUNI'
  "
)

muni_agency_names <- muni_agency_names %>% 
  mutate(first6 = str_sub(agency_num,1,6),
         first5 = str_sub(agency_num,1,5)) %>% 
  select(-minor_type)


# all agency names, numbers, and types
# includes TIF and non-TIF agencies
all_taxing_agencies <- DBI::dbGetQuery(
  ptaxsim_db_conn,
  "SELECT agency_num, agency_name, major_type, minor_type
  FROM agency_info
  "
)


all_taxing_agencies <- all_taxing_agencies %>%
  mutate(first6 = str_sub(agency_num,1,6),
         first5 = str_sub(agency_num,1,5)) 


all_taxing_agencies <- all_taxing_agencies %>% 
  left_join(muni_agency_names, by = c("first5", "first6")) %>% 
  rename(muni_name =  agency_name.y,
        muni_num = agency_num.y,
        agency_name = agency_name.x,
        agency_num = agency_num.x) 



# add taxing agency names and agency type to data table that has eav and extension values
agency_data <- right_join(agency_dt, all_taxing_agencies, by = c("first6", "first5", "agency_num") ) %>%
  mutate(first2 = str_sub(agency_num, 1,2),
         last2 = str_sub(agency_num,8,9),
         in_SSA = ifelse(minor_type == "SSA", 1,0),
         in_chicago = ifelse(str_detect(agency_name, "CHICAGO"),1,0)) %>%
  select(-c(cty_dupage_eav:cty_livingston_eav)) 

agency_data <- agency_data %>% 
  mutate(cty_cook_eav = as.numeric(cty_cook_eav), # EAV after exemptions
         total_final_levy = as.numeric(total_final_levy), # value that matches Clerk Office PDF for Levy
         cty_cook_eav = ifelse(cty_cook_eav < 1, NA, cty_cook_eav), # Code as missing so Log() doesn't make -Inf
         total_final_levy = ifelse(total_final_levy <1, NA, total_final_levy)) %>%
  mutate(log_eav = log(cty_cook_eav), 
         log_levy = log(total_final_levy))
         
```



```{r eval=FALSE, include=FALSE}
# loop practice
# calculates  the mean for every variable at once

output <- vector("double", ncol(agency_data))
names(output) <- names(agency_data)
for (i in names(agency_data)) {
  output[i] <- mean(agency_data[[i]], na.rm=TRUE)
}
output

# calculates the number of observations for each variable
output <- vector("double", ncol(agency_data))
names(output) <- names(agency_data)
for (i in names(agency_data)) {
  output[i] <- n_distinct(agency_data[[i]])
}
output

```

# Creating samples for models

## Homerule Munis: no TIFs or SSAs, no Chicago

> Also testing different model specifications in code below (standardized coefficients, robust standard errors)

- plm() uses the entity-demeaned OLS algorithm and does not report dummy coefficients.

- robust has multiple options: HC0 to HC5. Default is set to "HC3 BUT Stata default is to use "HC1"

[Command and package author site for summ() and table formatting](https://jtools.jacob-long.com/reference/summ.lm.html) 

model = "within" is the same as creating dummies for all agencies

```{r filtering-agencies}

homerule <- agency_data %>% 
  filter(major_type == "MUNICIPALITY/TOWNSHIP" & 
           in_SSA==0 &  # Not SSA
           home_rule_ind==1 &  # Homerule only
           in_chicago ==0 & # Not chicago
           (minor_type !=" TIF" & minor_type!="COOK")) %>% 
  select(year, agency_name, agency_num, 
         major_type, minor_type, cty_cook_eav, 
         total_final_levy,  total_final_rate,
         log_eav, log_levy
         ) 



agencies<- pdata.frame(homerule, index = c("agency_num", "year"))

pdim(agencies)

# doesn't matter, same results.
balanced_panel <- make.pbalanced(agencies,  balance.type = "fill")

#pdim(balanced_panel)

m1 = lm(log_levy ~ log_eav, data = homerule)

# not balanced panel, only has agency_name as factor
m2 <- plm(log_levy ~ log_eav, 
    data = agencies,
    index = c("agency_name", "year"), 
                   model = "within", effect = "individual")

# unbalanced panel, two way fixed effects
m3 <- plm(log_levy ~ log_eav + agency_name + year, 
    data = homerule,
    index = c("agency_name", "year"), 
                   model = "within", effect= "twoways")


model_names <- c("Naive:No FE", "Agency FE", "Agency & Year FE")
# coefs <- c("","") # to make names nicer

export_summs(m1, m2, m3, model.names = model_names, scale=TRUE, robust = "TRUE")
#export_summs(m1, m2, m3, model.names = model_names, scale=TRUE, robust = "HC1")

export_summs(m1, m2, m3, model.names = model_names)


vcovHC(m1, type = "HC1")
# obtain a summary based on clusterd standard errors 
# (adjustment for autocorrelation + heteroskedasticity)
coeftest(m1, vcov. = vcovHC, type = "HC1")
```
```{r year-dummies, eval=FALSE}
homerule <- homerule %>% mutate( year = as.factor(year),
                                 agency_name=as.factor(agency_name),
                                 agency_num = as.factor(agency_num))

#m1 <- lm(log_levy~log_eav + agency_name, data = homerule)

#m2 <- lm(log_levy~log_eav + agency_name + year ,data = homerule)

#export_summs(m1,m2, robust = "HC1", error_format ="({statistic}, p = {p.value})")

# r2 = .97 and .99 for the two models
# where year and agency_name are factors

```

### Heteroskedasticity

https://www.econometrics-with-r.org/10-5-tferaaseffer.html


Similar as for heteroskedasticity, autocorrelation invalidates the usual standard error formulas as well as heteroskedasticity-robust standard errors since these are derived under the assumption that there is no autocorrelation. When there is both heteroskedasticity and autocorrelation so-called heteroskedasticity and autocorrelation-consistent (HAC) standard errors need to be used. Clustered standard errors belong to these type of standard errors. They allow for heteroskedasticity and autocorrelated errors within an entity but not correlation across entities.

As shown in the examples throughout this chapter, it is fairly easy to specify usage of clustered standard errors in regression summaries produced by function like coeftest() in conjunction with vcovHC() from the package sandwich. Conveniently, vcovHC() recognizes panel model objects (objects of class plm) and computes clustered standard errors by default.

The null hypothesis for the Breusch-Pagan test is homoskedasticity.


```{r}
bptest(log_levy~log_eav + factor(agency_num) + factor(year), data = balanced_panel)
```
Controlling for heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator)

The `vcovHC` function estimates three heteroskedasticity-consistent covariance
estimators:  
• "white1" - for general heteroskedasticity but no serial correlation. Recommended for
random effects.   
• "white2" - is "white1" restricted to a common variance within groups. Recommended
for random effects.   
• "arellano" - both heteroskedasticity and serial correlation. Recommended for fixed
effects.   

```{r}
#coeftest(m1)
#coeftest(m1, vcovHC(fixed, method = "arellano"))
```




```{r }
# still includes cook county and other kind of weird agencies
no_homerule <- agency_data %>% 
  filter(home_rule_ind==0) #%>% select(-c(cty_dupage_eav:cty_livingston_eav))




```

## Schools

```{r}

schools <- agency_data %>% 
  filter(major_type=="SCHOOL")  %>% 
  select(agency_name, year, agency_num, log_levy, log_eav)
# group schools by minor type later!

# model wouldn't work until I selected only a few variables. in line above
plm(log_levy ~ log_eav, index = c("agency_name", "year"), model = "within",  effect= "twoways", data = schools)
```


```{r}
munis_homerule <- agency_data %>% 
  filter(minor_type=="MUNI" & home_rule_ind==1) %>% 
  mutate( agency_num = as.character(agency_num)) 

munis_no_homerule <- agency_data %>% 
  filter(minor_type=="MUNI" & home_rule_ind==0) %>% 
  mutate( agency_num = as.character(agency_num)) 
```

>  add heteroskedastic errors to models

`agency_data` table from ptaxsim does NOT include TIFs. 

```{r}
munis_homerule %>% 
  group_by(muni_name, year) %>%
  summarize(muni_levy = sum(total_levy),
            muni_eav = max(cty_cook_eav)) %>% 
  arrange(-year, muni_name)
```

```{r eval=FALSE}
tif_distribution <- DBI::dbGetQuery(
  ptaxsim_db_conn,
  "SELECT year, agency_num, agency_rate, tax_code_num, tax_code_distribution_pct
  FROM tif_distribution
  "
)

tif_crosswalk <- DBI::dbGetQuery(
  ptaxsim_db_conn,
  "SELECT year, agency_num_dist, agency_num_final
  FROM tif_crosswalk
  "
)


# 107,242 observations 
agency_fund <- DBI::dbGetQuery(
  ptaxsim_db_conn,
  "SELECT year, agency_num, fund_num, final_levy, final_rate, levy, loss_pct, levy_plus_loss, rate_ceiling, max_levy, prelim_rate, ptell_reduced_levy, ptell_reduced_ind 
  FROM agency_fund
  "
)

#156 fund types with their names and if the fund is statitorily capped or not.
agency_fund_info <- DBI::dbGetQuery(
  ptaxsim_db_conn,
  "SELECT fund_num, fund_name, capped_ind
  FROM agency_fund_info
  "
)

#agency_fund <- left_join(agency_fund, agency_fund_info)

#write_csv(agency_data, "taxing_agency_AWM_06012023.csv")
```

All of the data tables produced above are just stored data included in ptaxsim.

From agency.R code in ptaxsim Gitlab "The levy of each jurisdiction is reported by the Cook County Clerk's Office. URL here: <https://www.cookcountyclerkil.gov/service/tax-extension-and-rates>

Using the provided agency rate reports (from 2006 onward) for the sake of simplicity. These files also contain the total Cook County Equalized Assessed Value by agency."

PTELL = Property Tax Extension Law Limit.

Assessed value as equalized (AVE) = (assessed value \* equalization factor)

People can appeal their assessed values (assessed value is 10% of market value for residential)

Taxable Value of property = Equalized Assessed Value (EAV) = "assessed value as equalized"-exemptions

Note:

-   EAV from tax_bill() command is the EAV before exemptions are removed. Think of it like the "starting point" before it is manipulated at all.\
-   That is different than the EAV from the clerk office on PDFs (which is after exemptions.... and tifs?)

Exemptions lower tax bills by reducing the equalized assessed value of your property.

Taxing District tax levy / taxable value in the district = District Tax Rate

Number of taxing districts includes any district which has levied at some point in the past and has not been terminated by its governing jurisdiction plus TIFs

Chicago general composite rate does not include special districts such as mosquito abatement, special service areas, or home equity assurance district



## Descriptive Stats

Levy variables exploration:

-   Total Levy is what they asked for\
-   Total extension is what was actually collected?

Taxing District Tax Levy / Taxable Value in a Taxing District = District Tax Rate

Therefore, **TaxRateXLevy = TaxableValue**

TaxableValue = original EAV - exemptions - TIF increment




```{r}
# # diagnal line. Same variable
# agency_data %>% ggplot()+
#   geom_point(aes(x=as.numeric(total_levy), y=as.numeric(total_max_levy), col=major_type)) +
#   ggtitle("Total Levy and Total Max Levy")

agency_data %>% ggplot()+
  geom_point(aes(x=as.numeric(total_final_levy), y=as.numeric(total_max_levy), col=major_type)) +
  ggtitle("Total Final Levy and Total Max Levy")

agency_data %>% ggplot()+
  geom_point(aes(x=as.numeric(total_final_levy), y=as.numeric(total_reduced_levy), col=major_type)) +
  ggtitle("Total Final Levy and Total Reduced Levy")

agency_data %>% ggplot()+
  geom_point(aes(x=as.numeric(total_final_levy), y=as.numeric(total_ext), col=major_type)) +
  ggtitle("Total Final Levy and Total Extension \nSome Schools and Misc Agencies cross Cook Lines")
```

EAV exploration:

-   Summed EAV for each Agency - Michael's default way of thinking

-   cty_total_eav = all county EAVs summed together (LaSalle, Will, Cook, etc.)\

-   cty_cook_eav = eav after exemptions in cook county for a taxing district

Why are the other counties included in there? - Some taxing districts extend outside of cook county. cty_lasalle_eav would have eav that is in LaSalle County.

```{r}
agency_data %>% ggplot()+
  geom_point(aes(x=as.numeric(cty_total_eav), y=as.numeric(cty_cook_eav), col=major_type))+ggtitle("Total County EAV and Cook County EAV")
```

> To do: Make a binary variable for in_chicago for pins or tax_codes or agencies

Total Final Levy and Cook County EAV values:

```{r}
agency_data %>% ggplot()+
  geom_point(aes(x=total_final_levy, y=cty_cook_eav, col=major_type))+
  coord_flip()+
  ggtitle("Total Final Levy and Cook County EAV")

agency_data %>% 
  filter(year == 2021) %>%
  filter(minor_type == "MUNI" &
           in_chicago == "0") %>% # Chicago was massive outlier in the uppper right corner
  ggplot(aes(x=total_final_levy, y=cty_cook_eav, col=major_type, label = agency_name))+
  geom_point() +#
coord_flip()+
  ggtitle("Municipalities EAV & Levy")

agency_data %>% ggplot()+
  geom_point(aes(x=total_max_levy, y=cty_cook_eav, col=major_type))+
  coord_flip()+
  ggtitle("Total Max Levy and Cook County EAV")



agency_data %>% ggplot()+
  geom_point(aes(y=total_final_levy, x=cty_cook_eav, col=minor_type))


```

```{r}
agency_data %>% 
  filter(year== 2021) %>% 
  ggplot()+
  geom_point(aes(x=total_final_levy, y=cty_cook_eav, col=major_type))+ coord_flip()
  ggtitle("Major Types: Agency EAV vs Final Levy for 2021")

agency_data %>% 
  filter(year== 2021) %>% 
  ggplot()+
  geom_point(aes(x=total_final_levy, y=cty_cook_eav, col=minor_type))+ 
  ggtitle("Minor Types: Agency EAV vs Final Levy for 2021") + coord_flip()
```

## Munis only

```{r}
agency_data %>% 
  filter(year == 2021) %>% #2021 only
  
  # not in Chicago and municipalitys + Cicero
  filter(!(agency_num %in% c("030210000", "030210001", "030210002" ) ) 
         & (minor_type == "MUNI" | agency_num =="020060000")) %>%
  ggplot(aes(x=cty_cook_eav, y=total_final_levy, col=minor_type, label = agency_name))+
  geom_point() + coord_flip()

agency_data %>% 
  # not in Chicago and municipalitys + Cicero
  filter(!(agency_num %in% c("030210000", "030210001", "030210002" ) ) 
         & (minor_type == "MUNI" | agency_num =="020060000")) %>%
  ggplot(aes(x=cty_cook_eav, y=total_final_levy, col=agency_name, label = agency_name))+
  geom_point(alpha=.5) + coord_flip()+
theme(legend.position = "none")

agency_data %>% 
#Cicero only  
  filter(agency_num =="020060000") %>%
  ggplot(aes(x=cty_cook_eav, y=total_final_levy, col=agency_name, label = agency_name))+
  geom_point(alpha=.5) + coord_flip()+
theme(legend.position = "none")

agency_data %>% 
#Cicero only  
  filter(agency_num =="020060000") %>%
  ggplot(aes(x=year, y=total_final_levy, col=agency_name, label = agency_name))+
  geom_point(alpha=.5) +
theme(legend.position = "none")

agency_data %>% 
  filter(year == 2021) %>%
  filter(!(agency_num %in% c("030210000", "030210001", "030210002" ) ) 
         & (minor_type == "MUNI" | agency_num =="020060000")) %>%
  ggplot(aes(x=cty_cook_eav, y=total_final_levy, col=minor_type, label = agency_name))+
  geom_point()+ 
  geom_text(size = 2, vjust=-1.5) + coord_flip()
```

```{r}
muni_panel <- agency_data %>% 
  filter(!(agency_num %in% c("030210000", "030210001", "030210002" ) ) 
         & (minor_type == "MUNI" | agency_num =="020060000")) %>% 
  select(year, agency_name, agency_num, major_type, minor_type, cty_cook_eav, total_final_levy, log_eav, log_levy, total_final_rate
         ) 



## NExt two just switch X and Y 
plm(log_eav ~ log_levy, 
                   index = c("agency_name", "year"), 
                   model = "within", effect= "twoways",                    
    data = muni_panel) %>% 
  summary()


plm(log_levy ~ log_eav, 
                   index = c("agency_name", "year"), 
                   model = "within", effect= "twoways",                    
    data = muni_panel) %>% 
  summary()
```

## No TIFs or SSAs

Home rule and non home-rule:
```{r}
hr_notifs <- agency_data %>% 
  filter(in_SSA == 0 & minor_type != "TIF") %>%
  filter(home_rule_ind == 0)
  
nohr_notifs <- agency_data %>% 
  filter(in_SSA == 0 & minor_type != "TIF") %>%
  filter(home_rule_ind==1)

```

# Regressions - All Agencies

Simple model was log(levy) \~ log(base) where each observation is an agency-year and the base is the sum of that jurisdiction's taxable eav.

The coefficients would be interpretted as "a x% change in eav leads to a y% change in levy", which is also the elasticity.

```{r}
# tax base = log(levy)
# estimate the fixed effects regression with plm()


panel_data <- agency_data %>% select(year, agency_name, agency_num, major_type, minor_type, cty_cook_eav, total_final_levy, log_eav, log_levy, total_final_rate
         ) 


agencies<- pdata.frame(panel_data, index = c("agency_name", "year"))


# one way, individual effect within model.

levy_fe_mod <- plm(total_final_levy ~ cty_cook_eav,
                    index = c("agency_name"), 

                     model = "within",                  

                    data = agencies)
levy_fe_mod %>% summary



plm(log_levy ~ log_eav, 
                   index = c("agency_name", "year"), 
                   model = "within", effect= "twoways",                    
    data = agencies) %>% 
  summary()
```

## Other Stuff

The code that makes the tax_bill function looks like this:

```{r eval=FALSE}
tax_bill <- function(year_vec,
                     pin_vec,
                     tax_code_vec = lookup_tax_code(year_vec, pin_vec),
                     agency_dt = lookup_agency(year_vec, tax_code_vec),
                     pin_dt = lookup_pin(year_vec, pin_vec),
                     tif_dt = lookup_tif(year_vec, tax_code_vec),
                     simplify = TRUE) {
  # Basic type/input checking for vector inputs
  stopifnot(
    is.numeric(year_vec),
    length(year_vec) > 0,
    all(nchar(pin_vec) == 14),
    is.character(pin_vec),
    all(nchar(tax_code_vec) == 5 | is.na(tax_code_vec)),
    is.character(tax_code_vec),
    is.logical(simplify),
    length(simplify) == 1
  

```

It is possible to see exactly how much goes to Public Safety, Health Facilities, etc. By using the agency_fund_info datasets. Otherwise these funds are aggregated to a single category "COUNTY OF COOK."

-   Until today (June 1), I didn't know this was data existed. That's pretty neat.



```{r eval = FALSE, include = FALSE}
municipality_name, municipality_num, census_place_geoid
```

